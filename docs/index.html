<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Predicting Top Chart Songs Using Spotify Data</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Predicting Top Chart Songs Using Spotify Data</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Trevor Garner, Morgan McGuinn, Taylan Selman, Khushi Magiawala, Kira Pancha</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2020 CS 4641 Machine Learning: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Introduction/Background/Problem Definition -->
<h3>Introduction/Background/Problem Definition</h3>

We chose to look at music trends and charts in the United States because of our team’s collective love for music. Our goal is to produce a predictive model using supervised learning. Our product should be able to predict if a certain song will chart in the top 50 of Billboard’s Hot 100 based on certain features. Through this project, we’ll analyze how the world’s music taste has shifted over the last decade. If time permits, we may see which features affect the popularity of a song/track the most.
We hope that our predictive model will serve as a solution for upcoming artists trying to figure out what type of song does well on the Billboard Hot 100. Getting a song on this chart increases song and artist exposure: this can lead to record deals and a growth in sales. Record-labels could potentially use our product to reduce resource waste. By figuring out which songs and thus, artists, will chart, the record company can allocate more funds and marketing resources towards that specific release. 
Creating a predictive model using classification and regression algorithms using data from Spotify and Billboard’s API to analyze what song features are most prominent in the top 50 of Billboard’s H100.

<br><br>



<!-- Data Collection -->
<h3>Data Collection</h3>
We are dealing with two data sets: one from Billboard Hot 100 and one taken from Spotify’s API. 
The HotStuff.cvs (Billboard dataset) contains each song that made the Hot 100 chart. We use the peak_position feature to track whether a song charted in the top 50 or not.
The Billboard Hot 100 is based on sales, airplay, and total stream counts. This includes US based plays on YouTube (a song's music video). The specific formulas for each are a trade secret and thus, not available to evaluate or consider in our project. Instead, we use the Spotify data set to look for features specific to the song, and not consumer interaction with the song [cite this]. Spotify’s algorithm for calculating their features is also a trade secret and complicated, so we will not discuss the algorithms behind them. These features are calculated for every song that is published on their platform. Hence, we will be using this model to predict whether a song that has been published on their platform will chart in the top 50 of the Billboard Hot 100.
Here are descriptions for what some of our features mean (taken from data set description for Hot 100 Audio Features.xlsx and Spotify developer website and the HotStuff.cvs (Billboard dataset) description):
<br><br>
<!-- 2nd Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="spotifyfeatures.png">
</div>
Spotify’s algorithm for calculating their features is a trade secret and complicated, so we will not discuss the algorithms behind them. However, these features are calculated for every song that is published on their platform. Hence, we will be using this model to predict whether a song that has been published on their platform will chart in the top 50 of the Billboard Hot 100.
Some features that we hope to focus on are genre, tempo, energy, duration, acousticness and danceability. To account for the fact that Spotify launched in 2008, we will only use the data points from 2008 – 2019 in the Billboard dataset.
Furthermore, because our data set encompasses all songs that charted on the Billboard Hot 100. We look at what makes a song chart in the top 50. This allows us to roughly have 50% of data points that did not chart and 50% that did. Thus, we will be able to see favorable and unfavorable feature values.

<br><br>
<!-- Data Cleaning: -->
<h3>Data Cleaning:</h3>
To prepare our data for the learning models, we filtered out any unnecessary variables/features and joined the two data sets on song id. Both the Billboard and spotify data sets originally included thousands of entries. However, once we combined the two data sets and removed duplicate, irrelevant and incomplete entries, our data set was reduced to around 850 entries. A sample of our code for filtering entries from the Billboard set is below:

<br><br>
index_names = billboard_df[ (billboard_df['WeekID'] < 2008) | (billboard_df['WeekID'] > 2019) ].index
<br><br>
billboard_df.drop(index_names, inplace = True)
<br><br>
billboard_df.drop_duplicates(subset='SongID', inplace=True)
<br><br>
billboard_df.dropna(axis=0, inplace=True)
<br><br>

Here, we remove entries that did not chart between 2008 and 2019, entries with the same song name, and entries that have any null feature fields. We chose the years 2008 and 2019 because Spotify only came out in 2008, and we know that the data for 2019 is complete, whereas data for 2020 may be updated at different times in the Billboard and Spotify datasets since the year is not over. Dropping duplicate songs is necessary because we only want to consider one time the song charted. This specific script only keeps the first instance of each song in the dataset, which worked for our purposes because there is a column that has the song’s peak position. Therefore, we could’ve kept any arbitrary entry for any given song. Finally, we opted to deal with null columns by removing any rows that had null values. Since there were not many rows that fell under this category, we chose this approach because the information loss would not be too large. However, there are other approaches that we considered that would’ve better dealt with information loss, such as filling in null fields with the most frequent value for that feature, or actually predicting the missing values.
The reduction in size of our data set is logical. The spotify data set included songs not included in the billboard set. The billboard data set included an entry for every single week a song had charted, so we chose only to include the peak position of the song.

<br><br>
<!-- Feature Selection -->
<h3>Feature Selection</h3>
<!-- Graph Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="graph.png">
</div>
We ran PCA on our data to figure out which features were more significant and if there was a significant variance among data points within the features. We condensed the 8 features into two overall features (one for Top 50 songs, and one for songs not in the Top 50) and plotted them out. As seen in the plot however, there was no distinction between the feature values of songs in the Top 50 and songs not in the Top 50, so there isn’t a significant enough variance among data points within the features for us to say one is more significant than the other or that a feature is significant at all. For this reason, we ended up not using PCA to reduce our feature dimensions, but some of the models we tested have their own, built in feature engineering, such as Neural Networks (NN). 


<br><br>
<!-- Methods -->
<h3>Methods</h3>
We chose various classification techniques on our data set. These include Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), Nearest Neighbors (KNN), and Neural Net (NN). Linear and Quadratic Discriminant Analysis were chosen because these algorithms can reduce the dimensionality of our data, similar to PCA, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes (LDA). Since our data had low variance scores we decided to implement K-Nearest Neighbors (KNN) which assumes our data is in close proximity to each other. Then we used the calculated groups to predict which label our new data falls under. Lastly, we implemented a Neural Network with 8 input nodes, 2 hidden layers, and 1 output node to predict our new data. All of these classification techniques come with their own pros and cons which is why we incorporated each of them. Thus, giving us a good perspective on which algorithms work best with our data set. 
<br><br>



<!-- Results -->
<h3>Results</h3>
To obtain our results we used an 80/20 train test split. Where 80% of the data was used to train each model and 20% of the data was used to access the accuracy of each model. 
For our NN we used a batch size of 10 as we had around 850 data samples, and we achieved the best results while using a batch size of 10. 
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="results.png">
</div>

It is evident that our Neural Net performs the best with our data, garning a accuracy of 76.13%. The worst being Quadratic Discriminant Analysis with an accuracy of 55.23%. There are several factors as to why our accuracy values are better than our baseline results but not high achieving. The main reason is because each feature in our data set has a low amount of variance. This causes our data to become “clumped” together thus making any classification harder. This is especially seen in feature dimensionality reduction algorithms such as PCA (reference the feature selection section), LDA, and QDA. Which gives us the reason as to why the accuracy values of LDA and QDA are much lower than our neural network. Overall, we are happy with the results so far with an accuracy rating of 76.13%, but we hope to increase this by at least 5% in the future.
<br><br>

<!-- Discussion -->
<h3>Discussion</h3>
Preliminary results show relatively robust prediction accuracy, well above the baseline (naive) approach, with the Neural Net accuracy peaking at 76.13%.  We believe accuracy is a bit worse than comparable approaches (achieving 80-90% using similar models as well as bagging, which we can add in future iterations) due to low feature variance, which can perhaps be remedied with more data cleaning and feature selection.  Another possible explanation for the accuracy loss is that listener preferences change over time––a song that charts in the top 50 in 2008 might not in 2015.  In future iterations, we would account for this impact using a Long Short-Term Memory (“LSTM”)  model.
<br><br>




  <hr>
  <footer> 
  <p>© Trevor Garner, Morgan McGuinn, Taylan Selman, Khushi Magiawala, Kira Pancha</p>
  </footer>
</div>
</div>

<br><br>

</body></html>